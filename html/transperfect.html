<!DOCTYPE html>
<html>
<head>

<meta charset="UTF-8" />

<title>High Velocity Ingest/Digest</title>
<link rel="stylesheet" type="text/css" href="index.css">

<script type="text/javascript">
__legal__ = { owner: "Transperfect" };
</script>

<!-- jquery is used only for $(document).ready(fun); legacy capability -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js">
</script>

<script src="index.js">
</script>
</head>

</body>

<article>

  <aside id="define" hidden>
    <![CDATA[
    {jdl=Jonathan D. Lettvin}
    {al=Andrew Leonard}
    {jk=Joe Kuefler}
    {pf=High Velocity Ingest/Digest}
    {tp=TransPerfect&copy;}
    ]]>
  </aside>

  <header>
    ==&pf;==
    ===faster parsing of web content===
    =====reduce use of branches and functions=====
    Authors: &jk; &al; &jdl;
  </header>

  <nav></nav>

  <section id="Home">
    !C!url(page modeled on Heilmeier catechism)
    @http://www.design.caltech.edu/erik/Misc/Heilmeier_Questions.htmlurl!C!
    ___
    =====GOAL!=====
    &tp; ingest/parse time dominates the processing time for translation.
    &jk;, &al;, and &jdl; are tasked with
    significantly reducing the proportion of time taken by ingest/parse.
    ___
    =====CURRENTLY=====
    ...The existing code uses algorithms that we need to understand.
    We need to be able to run it autonomously for performance measurements.
    ___
    =====NEW APPROACH=====
    ...Known (but rarely discussed) methods for writing compilers and DSLs
    can be immediate and significant stream velocity optimizers.
    It is reasonable to say that these methods,
    taken to their extreme,
    are the !=!_ideal_!=! balance between
    commonly known techniques and high-performance.
    ___
    =====DIFFERENCE=====
    ...We expect that the existing code does not use these methods,
    so we expect that side-by-side comparison of
    the proposed approach with the existing approach
    will yield measurable improvements in velocity.
    If this is true, even with only 10% improvement,
    it is likely worth pursuing.
    Part of the reason is that the proposed techniques
    are mathematically provable and very easy to test and maintain.
    ___
    =====RISK/BENEFIT=====
    * If the measurements do not show improved velocity
    then the time used to develop the measurements will have been wasted.
    * If the measurments do show improved velocity
    then translation throughput can be increased.
    ___
    =====COST=====
    Full-time development by &al; and &jdl; will cost
    the fully-loaded salaries for two human resources
    who could otherwise have retired more tickets
    in the main development activities of an OLE.
    ___
    =====ROUGH TIME ESTIMATE=====
    Because of existing experience of &jdl;,
    the clear talent of &al;,
    and the direction, insight, and occasional help by &jk;
    This project will likely show promise within 2 months,
    be demonstrable in 4-6 months,
    and could become fully integrated and sole technology
    for &pf; by as little as 9 months
    ___
    =====MILESTONES=====
    * Autonomous timeable existing algorithm
    * graphviz map for intended new algorithm (and swap-out)
    * Full description of each needed element
    * Proposed implementation details (table-driven, spaghetti, goto...)
    * TDD tests against which to fail
    * Implementations to retire failed tests
    * Side-by-side timing comparison with existing algorithm
    * Integration into a virtual machine
    * Rigorous smoke-tests
    * Declaration of direction (retire old or new algorithm)
  </section>

  <section id="Theory">
    ___
    =====Details!=====
    ___
    ======Transperfect needs to change how it lexes HTML:======
    * It must be as fast as theoretically possible
    * It must be "relaxed" (forgiving of structural error)
    * It must be configurable for different versions and styles of HTML
    * It must handle all 13 Unicode byte patterns:
    ** ASCII
    ** UTF8
    ** UTF16 12 21
    ** UTF32 1234 4321 2143 3412
    * It must handle BIG5 and a variety of other non-Unicode formats
    ___
    ======How does one set direction with goals like this?======
    * Throw caution to the wind
    * Trust compiler algorithms; tried-and-true optimized throughput
    * Use techniques with known speed advantages; flouting advice
    ** "GOTO Considered Harmful" !=WRONG!=!
    ** Avoid spaghetti-code !=WRONG!=!
    ** Use OOP !=worth questioning=!
    ======Can "FASTER" be achieved without measurement?======
    ...Just as Order assessments offer advantages when choosing algorithms
    there are other assessments that can be made without hesitation.
    Here are a few:
    * A branch (if, switch, for, while) is expensive.  Minimize use of them.
    * Function calls are expensive.  Minimize use of them.
    * Visit a character exactly once, change state, and move forward.
    * "Pattern Matching Considered Harmful".
    * Minimize use of dictionaries.  Use table indexing wherever possible.
    * Never copy characters.  Leave in place, terminate, and use as a string.
    ...&jk; proposed that building a DOM is inappropriate:
    * Some pages can be quite large.
    * Most pages have formatting errors.
    * A SAX-like mechanism which operates as far as it can is superior.
  </section>

  <section id="Acquisition">
    ___
    ...Characters should be buffered in in efficient-size blocks,
    processed, then discarded without history.
    Unicode characters are 21-bit "codepoints".
    Codepoints are delivered in streams
    for which the stream-type must be identified.
    If a BOM (Byte Order Mark) is present, this is easy,
    but if absent, there are still fairly trivial ways
    to identify a stream from the bytes early in the stream
    (I have existing code to do this).
    ___
    ...If the stream is delivered in the wrong "endian" style
    byte-swapping should be done, but branches should be avoided
    (I have existing code to do this).
    * !url Endian.c implements byte re-ordering for opposite endian data
    @https://github.com/jlettvin/Unicode/blob/master/c/Endian.c
    url!
    * !url Endian.h
    @https://github.com/jlettvin/Unicode/blob/master/c/Endian.h
    url!
    ___
    ...In UTF8, these are reconstructed by a series of
    shift/mask operations on byte streams.
    The typical branch-point style of decoding UTF8 streams is heavy.
    Decoding UTF8 streams can be done without branches
    (I have existing code to do this).
    * !url UTF8.c implements translations (UTF8 &lt;-&gt; int)
    @https://github.com/jlettvin/Unicode/blob/master/c/UTF8.c
    url!
    * !url UTF8.h
    @https://github.com/jlettvin/Unicode/blob/master/c/UTF8.h
    url!
    ___
    ...The same can be done for all stream-types.
    Characters have classifications (in Unicode)
    to aid in choosing state transitions.
    These classifications must be identifies
    without function-call or branch point
    (I have existing code to do this).
    * !url
    Classify.c implements a Unicode 9.0 compliant classification table
    @https://github.com/jlettvin/Unicode/blob/master/c/Classify.c
    url!
    * !url Classify.h
    @https://github.com/jlettvin/Unicode/blob/master/c/Classify.h
    url!
    !V
    V!
  </section>

  <section id="Identification">
    ___
    ======Converting streams to internal characters can be fast======
    ... There is no need for function calls or branch points
    other than buffering input on its way to being delivered to the lexer.
    ___
    ...The first 256 bytes of an HTML page contains
    enough information to identify how it is encoded.
    Whether it is ASCII, UTF8, UTF16, UTF32, BIG5, or some other encoding;
    specific non-branching, non-calling lexers/parsers
    can be produced to match the stream and convert
    the incoming bytes into a stream of 32 bit INTs
    suitable for shared internal processing via LR parsing.
    ___
    ...The first operation must always be this identification.
    The first buffer of 256 bytes must always be reprocessed by
    the identified parser.
    All the parsers shall compile to a single object file (.o or .obj)
    such that all linkage is trivial and local
    decreasing any burdens imposed by the linker mechanism.
    The dekaf model is useful in this regard.
    Each parser may be fully implemented in a header file
    which is included in a .c or .cpp file inside a structure
    used to separate the separable parser parts kept in the header file.
  </section>

  <section id="Lexing">
  </section>

  <section id="LR parsing">
  </section>

  <section id="Emit">
  </section>

  <section id="Log">
  </section>

  <section id="Demos">
    * !url Unicode service functions (under development)
    @https://github.com/jlettvin/Unicode/blob/master/c/Classify.c
    url!
    * !url (i.e. UTF8.c implements translations UTF8 &lt;-&gt; 32-bit int)
    @https://github.com/jlettvin/Unicode/blob/master/c/UTF8.c
    url!
  </section>

  <section id="TODO">
    * Convert my existing code as re-deployed for the new lexer
    * Use TDD !/Rigorously/! to demonstrate layered approach to goal
  </section>

  <footer></footer>
</article>

</body>
</html>
