This C language optimized library offers a set of functions designed to enable development of D\+SL lexing/parsing at very high performance while being as high quality as is reasonable without obfuscating what is done and why the implementation works.

The principle insight is that G\+O\+TO and table-\/driven transforms outperform architectures which test for values and take alternate code paths. G\+O\+TO and table-\/driven are considered unusual in the popular development community. \href{http://eli.thegreenplace.net/2012/07/12/computed-goto-for-efficient-dispatch-tables}{\tt Here is an article}.

The code is designed to be very short and tight. It makes use of the gnu compiler stack where computed G\+O\+TO has been a standard extension for many years, where other compilers have no support for this efficiency.

A benefit of the use of G\+O\+TO in this library is that the likely code path has been prefetched into the processor cache (so there is no flushing of the cache for either of the two directions the G\+O\+TO may take).

The general approach to table-\/driven paths is explained in the main R\+E\+A\+D\+M\+E.\+md which is in the parent directory to this library source.

This directory will be the primary area for development of a shared Python library because C\+F\+FI is less friendly to C++ than it is to C function entrypoints.

valgrind does not identify any memory leaks resulting from the code in this library There are memory leaks in the test modules due to implementation errors in the standard library.

\subsection*{Dependency graph}



\subsection*{Points of Interest}

\subsubsection*{U\+T\+F8 $<$-\/$>$ char32\+\_\+t}

Python libraries for handling U\+T\+F8 and full-\/width Unicode codepoints are fraught with pitfalls for the everyday developer. This library offers a simple pair of symmetric operations which convert between U\+T\+F8 encoded strings (the default most popular Unicode format) and string32 (the necessary ordinal size for internal Unicode).

Were this implementation not available, here are issues to consider\+:
\begin{DoxyItemize}
\item Recompile Python to handle 21 bit codepoints rather than only 16 bit
\item Import libraries for which the encode/decode interface is richer
\item Run additional tests to guarantee that the imports perform fully
\item Additional concerns not listed here...
\end{DoxyItemize}


\begin{DoxyCode}
1 # Here is all that will be needed to use these library functions:
2 
3 with open("UTF8encoded\_source.txt", "rb") as source:
4     asUTF8 = source.read()
5     UTF8\_to\_u32string(asUTF8, asU32String)
6 
7 # asU32String is an array of 32 bit codepoints.
8 
9 with open("UTF8encoded\_target.txt", "wb") as target:
10     u32string\_to\_UTF8(asU32String, asUTF8)
11     target.write(asUTF8)
\end{DoxyCode}


\subsubsection*{Generate refactored Classify.\+c \hyperlink{_classify_8h_source}{Classify.\+h}}

These files are now generated from Derived\+General\+Category.\+txt by gen\+\_\+\+Classify.\+py.

\subsection*{T\+O\+DO}


\begin{DoxyItemize}
\item Improve unit testing
\item Clarify typedefs in \hyperlink{_unicode_8h_source}{Unicode.\+h}
\item Find a worthy gprof for Mac OS X
\item Continue port to linux
\item Eliminate warnings on linux
\end{DoxyItemize}

\subsubsection*{T\+O\+DO Unicode.\+c}


\begin{DoxyItemize}
\item Improve Tests of U\+T\+F8 $<$-\/$>$ char32 transforms in test\+\_\+\+Unicode.\+c
\end{DoxyItemize}

\subsubsection*{T\+O\+DO gcov}


\begin{DoxyItemize}
\item Improve coverage
\end{DoxyItemize}

\subsubsection*{T\+O\+DO Classify}


\begin{DoxyItemize}
\item Find core dump on linux 
\end{DoxyItemize}